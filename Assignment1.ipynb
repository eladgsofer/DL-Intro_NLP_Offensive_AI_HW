{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 1. Music Century Classification\n",
        "\n",
        "**Assignment Responsible**: Natalie Lang | Students: Elad Sofer & Raviv Ilani.\n",
        "\n",
        "In this assignment, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI\n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## Question 1. Data (21%)\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "Since you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together.\n",
        "\n",
        "To process and read the data, we use the popular `pandas` package for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http.://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "EY6PrfV4zGq_",
        "outputId": "7bb30e05-98e7-4c1c-f18c-5d071b1f6b8d"
      },
      "source": [
        "load_from_drive = True\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/gdrive/Shareddrives/DNN_Elad_Raviv/YearPredictionMSD.txt'\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6763d8e37831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mt_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mx_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"var%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m91\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_label\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/Shareddrives/DNN_Elad_Raviv/YearPredictionMSD.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Write your explanation here:\n",
        "#if we have the same artist showinmg in both the test set and the training set the system might be compromised in identifying\n",
        "# a perticular artist's features instead of being as abstract as possible, risking in overfiting\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zmZk6ezGrC"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZy6brwzGrC"
      },
      "source": [
        "# Write your explanation here\n",
        "\n",
        "# In order to validate our model which is based on training set, we need to represent the training & testing in the same way. assuming\n",
        "# it came from the same X random variable.\n",
        "\n",
        "# In theory, the main requirement for the optimized model to generalize is to have\n",
        "# the data set comprised of i.i.d. samples, which implies that the empirical risk, means, variance converges to their\n",
        "# true values as the training set size grows by the law of large numbers.\n",
        "\n",
        "# In that manner, if we assume all of our data (testing & training sets) was generated from a certain X random variable\n",
        "# Ideally, the training set and the testing set will have the same variation and means. (for a big enough testing and training sets)\n",
        "# Although, in practice it's not the case. because the test set usually is much more smaller (so the law of large numbers do not apply, and anomalies can occur as well)\n",
        "# so the means/variance of the testing set could be far from the true value.\n",
        "\n",
        "# We can't use a different means to the validation set, otherwise it could lead that the validation came from a different random variable.\n",
        "# and the main requirments for the optimized model won't occur."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Write your explanation here\n",
        "# We divide our train data into 3 sets validation set, training set and test set. the validation set\n",
        "# target is to estimate the performance of our model in each iteration. we need the validation set\n",
        "# in order to examine which hyper parameters are giving us the best model. this is why we track our model\n",
        "# behaviour in each iteration. while tracking the performance, we will choose the best model upon it's performance, although\n",
        "# it's performance is relying upon the validation set, so in case we encounter overfitting (upon the validation set) we want\n",
        "# to test our NN upon a different dataset. and use it as a majorment to avoid overfitting. we want to distinguish between\n",
        "# the dataset for choosing the hyper parameters and the dataset for testing the overall NN behaviour.\n",
        "# if we use the dataset during the training, we will be bias to choose our model upon that set as well.\n",
        "# so the final test won't be effective (because we can encounter overfitting upon the test set as well)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6BA_s-kzGrD"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def cross_entropy(t, y):\n",
        "  return -t * np.log(y) - (1 - t) * np.log(1 - y)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by:\n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD"
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
        "  \"\"\"\n",
        "  # Your code goes here\n",
        "  return sigmoid(np.dot(w,np.transpose(X))+b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYepbUZlqtWE"
      },
      "source": [
        "pred(np.zeros(90), 1, np.ones([5, 90]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE"
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dLdw and dLdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "  Postcondition: np.shape(dLdw) = (90,)\n",
        "           type(dLdb) = float\n",
        "  \"\"\"\n",
        "\n",
        "  # Your code goes here\n",
        "  N= np.shape(X)[0]\n",
        "  return (1/N)*np.matmul(np.transpose(X), y-t), (1/N)*(np.sum(y-t))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPRGM3BjKe2"
      },
      "source": [
        "# **Explenation on Gradients**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfmPVdsg0eX"
      },
      "source": [
        "**Add here an explaination on how the gradients are computed**:\n",
        "\n",
        "Write your explanation here. Use Latex to write mathematical expressions. [Here is a brief tutorial on latex for notebooks.](https://www.math.ubc.ca/~pwalls/math-python/jupyter/latex/)\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "L = -t * log(t) -(1-y) * log(1-y) \\end{equation}\n",
        "\\begin{equation} y = sigmoid(z) = \\frac{1}{1-e^{-z}} \\end{equation}\n",
        "\\begin{equation} z^i = ({\\bf w}^T {\\bf x^i} + b),\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{dy}{dw_j} = \\sigma({\\bf w}^T {\\bf x^i} + b)(1-\\sigma({\\bf w}^T {\\bf x^i} + b))x_j = y(1-y)x_j\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{dl}{dy} = \\frac{-t}{y}+\\frac{1-t}{1-y}\n",
        "\\end{equation}\n",
        "\n",
        "$ \\bigtriangledown_w L = \\frac{1}{N} \\sum_{i=1}^{N}\\bigtriangledown_wl= \\frac{1}{N} \\sum_{i=1}^{N}\\frac{\\partial l}{\\partial w} ==\\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial l}{\\partial y} \\frac{\\partial y}{\\partial w} =\\frac{1}{N} X^T(\\frac{-t}{y}+\\frac{1-t}{1-y})y(1-y) = \\frac{1}{N}X^T(y-t)$\n",
        "\n",
        "$ \\bigtriangledown_b L = \\frac{1}{N} \\sum_{n=1}^{N}\\bigtriangledown_bl= \\frac{1}{N} \\sum_{n=1}^{N}\\frac{\\partial l}{\\partial b} =\\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial l}{\\partial y} \\frac{\\partial y}{\\partial b} =\\frac{1}{N} \\sum_{n=1}^{N}(\\frac{-t}{y}+\\frac{1-t}{1-y})y(1-y) = \\frac{1}{N} \\sum_{n=1}^{N}(y-t) $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF"
      },
      "source": [
        "# Your code goes here\n",
        "h = 10e-4\n",
        "X=np.array([4])\n",
        "t=np.array([3])\n",
        "w=np.array([3])\n",
        "b=np.array([8])\n",
        "\n",
        "y=pred(w, b, X)\n",
        "y_x_plus_h=pred(w, b+h, X)\n",
        "\n",
        "dLdW, dLdB = derivative_cost(X, y, t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "r1 = dLdB\n",
        "r2 = ((cross_entropy(t, y_x_plus_h) - cross_entropy(t, y))/h)[-1]\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "### Part (d) -- 7%\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF"
      },
      "source": [
        "# Your code goes here. You might find this below code helpful: but it's\n",
        "# up to you to figure out how/why, and how to modify the code\n",
        "\n",
        "h = 10e-4\n",
        "X=np.array([4])\n",
        "t=np.array([3])\n",
        "w=np.array([3])\n",
        "b=np.array([8])\n",
        "\n",
        "y=pred(w, b, X)\n",
        "y_x_plus_h=pred(w+h, b, X)\n",
        "\n",
        "dLdW, dLdB = derivative_cost(X, y, t)\n",
        "\n",
        "\n",
        "r1 = dLdW\n",
        "r2 = ((cross_entropy(t, y_x_plus_h) - cross_entropy(t, y))/h)[-1]\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### Part (e) -- 7%\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent.\n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(w0, b0, mu=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - mu as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        "\n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "  \"\"\"\n",
        "  global train_xs, val_ts, train_norm_xs, val_norm_xs, train_ts\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = 0\n",
        "  loss_array_val = []\n",
        "  VAL_LENGTH = len(val_ts)\n",
        "\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    # shuffle the training set\n",
        "    reindex = np.random.permutation(len(train_xs))\n",
        "\n",
        "    # features vector\n",
        "    train_xs = train_xs[reindex]\n",
        "    train_norm_xs = train_norm_xs[reindex]\n",
        "    # year vector - labels\n",
        "    train_ts = train_ts[reindex]\n",
        "\n",
        "    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs[i:(i + batch_size)]\n",
        "      t = train_ts[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      y = pred(w, b, X)\n",
        "\n",
        "      # update w and b\n",
        "      dLdW, dLdB = derivative_cost(X, y, t )\n",
        "      b = b - mu*dLdB\n",
        "      w = w - mu*dLdW\n",
        "\n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "\n",
        "      # compute and print the *validation* loss and accuracy\n",
        "      if (iter % 10 == 0):\n",
        "        val_cost = 0\n",
        "        val_acc = 0\n",
        "        y_val = pred(w, b, val_norm_xs)\n",
        "        for i_t, i_y in zip(map(lambda x: x[0], val_ts), y_val):\n",
        "          y_, t_ = np.array([i_y]), np.array([i_t])\n",
        "          cost_i = cost(y_[0], t_[0])\n",
        "          if not np.isnan(cost_i) and not np.isinf(cost_i):\n",
        "\n",
        "            val_cost += cost_i/VAL_LENGTH\n",
        "\n",
        "          val_acc += get_accuracy(y_, t_)/len(val_norm_xs)\n",
        "        loss_array_val+=[val_cost]\n",
        "        print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (\n",
        "                iter, val_acc * 100, val_cost))\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        x_axis = np.arange(10, max_iters + 10, 10)\n",
        "        plt.plot(x_axis, loss_array_val, 'r-', label=\"loss val\")\n",
        "        #plt.plot(x_axis, loss_array_train, 'g-', label=\"loss train\")\n",
        "        plt.legend()\n",
        "        plt.title('Validation set Loss vs Iteration mu={mu}'.format(mu=mu))\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "        break;\n",
        "\n",
        "      # Think what parameters you should return for further use\n",
        "\n",
        "  return w, b\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### Part (f) -- 7%\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n",
        "Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH"
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "# Write your code here\n",
        "mus= [0.01, 0.5, 100 ]\n",
        "for mu in mus:\n",
        "  run_gradient_descent(w0,b0, mu)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44GmUZ12o-g"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "\n",
        "\n",
        "we will get the best results when the network's parameters converge to a global minima of cost fuction. when we use a too large learning rate the parameters might jump over minimum whithout ever converging to it resulting in a resulting in \"overshooting\". too small of a learning rate might take too long to converge and risiking getting stuck in a local minima without ever reaching the global one. this can be shown in the figures above where a small learning rate gives a a linear slow a steady convergong graph while the too large learning rate jumps all over.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### Part (g) -- 7%\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI"
      },
      "source": [
        "\n",
        "def get_optimal_rand(w,b,size):\n",
        "  mu = [np.random.uniform(0, 1) for i in range(size)]\n",
        "  batch= [np.random.randint(1, len(train_xs)) for i in range(size)]\n",
        "\n",
        "  val_acc_temp =0\n",
        "\n",
        "  for i in range(1,size):\n",
        "    print(\"mu:\", mu[i], \"batch size:\", batch[i] )\n",
        "    w , b = run_gradient_descent(w0, b0, mu[i], batch[i] , max_iters=100)\n",
        "    y_val=pred(w,b,val_norm_xs)\n",
        "    val_acc = get_accuracy(y_val,val_ts)\n",
        "    if (val_acc_temp < val_acc) :\n",
        "      val_acc_temp = val_acc\n",
        "      w_opt, b_opt = w , b\n",
        "\n",
        "  return w_opt, b_opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmasp7vkTTXg"
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "w_opt, b_opt = get_optimal_rand(w0,b0,100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237LgOJhkqVL"
      },
      "source": [
        "#optimal b and w after runnig the program:\n",
        "\n",
        "w_opt= [ 1.35127201e+00, -1.02483271e+00, -4.62664721e-01,  4.98761494e-01,\n",
        "       -1.83756235e-01, -1.01668509e+00, -2.44870956e-01, -1.71661277e-01,\n",
        "       -1.83500582e-01, -1.01121757e-01, -4.87551676e-01,  1.45309989e-01,\n",
        "        3.46325967e-01,  1.58830071e-01, -1.11276385e-01,  1.79039040e-02,\n",
        "       -3.76774395e-02,  2.24750243e-01,  1.04717143e-01, -1.66887331e-01,\n",
        "       -1.06112210e-01,  4.49302646e-01,  7.33274469e-01,  3.47900808e-02,\n",
        "       -2.14784138e-01,  8.45031104e-02,  2.01655373e-01,  3.88160789e-02,\n",
        "       -4.31162072e-02, -1.77127620e-04, -1.50487152e-02, -6.03851996e-02,\n",
        "       -8.33959250e-03, -3.20592670e-03,  9.11464889e-02, -1.55892428e-01,\n",
        "       -8.98920542e-02,  9.82273371e-02,  4.44954214e-02, -5.75755562e-02,\n",
        "       -9.78038715e-02, -5.18041810e-02,  1.24456419e-03, -7.55342563e-04,\n",
        "        2.28864582e-02,  2.11193808e-01,  5.70056403e-03, -6.50813278e-02,\n",
        "        7.62481244e-02, -2.46316410e-02,  1.81390432e-02, -2.57190110e-03,\n",
        "        9.68082799e-02,  1.31798828e-03, -7.00420397e-02,  3.11951617e-02,\n",
        "       -1.41470782e-01,  1.63484051e-01, -7.13530454e-02, -6.84557969e-02,\n",
        "       -6.80627242e-02, -2.61846079e-02, -3.27865150e-01,  1.02505988e-01,\n",
        "       -8.39863837e-02, -9.76252384e-03,  3.76187766e-02,  3.55112137e-02,\n",
        "       -2.08128549e-02, -8.57730226e-02, -1.03894421e-01,  7.54514031e-02,\n",
        "       -8.58458958e-02,  4.09614802e-02,  7.16216533e-02,  2.54177544e-02,\n",
        "        1.20250047e-03, -1.41234470e-01, -4.96233772e-02,  3.69498001e-02,\n",
        "       -9.39100397e-02,  6.42013465e-02,  7.55807980e-02, -3.57901797e-02,\n",
        "        8.66354633e-02,  1.95073423e-03,  3.18231886e-02, -3.73210110e-01,\n",
        "       -2.32549082e-02,  7.25987919e-02]\n",
        "\n",
        "b_opt= 0.3119357591087439\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ5eVoQvCbIh"
      },
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "too small of a learning rate resulting in a liner graph\n",
        "while good hyperparameters give us an exponantial decaying graph:\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZt7_932zX2"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        ">optimal b and w were chosen by randomly choosing different learning rates and batch sizes. it seems that the 73% accuracy limit cannot be breached implying that maybe more layers are needed for better results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### Part (h) -- 15%\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI"
      },
      "source": [
        "# Write your code here\n",
        "w=w_opt\n",
        "b=b_opt\n",
        "y_train=pred(w,b,train_norm_xs)\n",
        "y_val=pred(w,b,val_norm_xs)\n",
        "y_test=pred(w,b,test_norm_xs)\n",
        "train_acc = get_accuracy(y_train,train_ts)\n",
        "val_acc = get_accuracy(y_val,val_ts)\n",
        "test_acc = get_accuracy(y_test,test_ts)\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZa1u6920M3"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        ">  we can see the three are just about the same with a slight advantage to\n",
        "the val_acc. obviously due to the fact that the optimal b and w were chosen by optimaizing the val_acc.\n",
        "test a bit lower than the train due to the fact that the test was on unseen samples.\n",
        "nonetheless we can see that our network has managed to get a good generalization of the data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eP2Yh1zGrI"
      },
      "source": [
        "### Part (i) -- 15%\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LCfAa1zGrJ"
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model= LogisticRegression()\n",
        "model.fit(train_norm_xs,train_ts)\n",
        "train_acc = model.score(train_norm_xs,train_ts)\n",
        "val_acc = model.score(val_norm_xs,val_ts)\n",
        "test_acc = model.score(test_norm_xs,test_ts)\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n",
        "\n",
        "\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqucdV923tG"
      },
      "source": [
        "**This parts helps by checking if the code worked.**\n",
        "**Check if you get similar results, if not repair your code**\n"
      ]
    }
  ]
}