{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3275ace2"
      },
      "source": [
        "# Exercise 1 - Question 1 (Language Models)"
      ],
      "id": "3275ace2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98eb6b6c",
        "outputId": "c99729e6-f4c0-4563-d4b1-9454b56f0733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "#Requirements: to be installed in the virtual environment\n",
        "# Elad Sofer 312124662 and Tomer Shaked 315822221\n",
        "\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install numpy\n"
      ],
      "id": "98eb6b6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272c0a7c"
      },
      "source": [
        "<a name=\"ngram_lm\"></a>\n",
        "## Task A: Data Exploration"
      ],
      "id": "272c0a7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "066c0427"
      },
      "source": [
        "\n",
        "Let's use an unsupervised dataset (raw corpus) to evaluate language models' perplexity. We use Huggingface's `datasets` library to download needed datasets.\n",
        "\n",
        "\n",
        "Here we use the `Penn Treebank` dataset, featuring a million words of 1989 Wall Street Journal material. The rare words in this version are already replaced with `<unk>` token. The numbers are also replaced with a special token. This token replacement helps us to end up with a more reasonable vocabulary size to work with.\n"
      ],
      "id": "066c0427"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69afb9de",
        "outputId": "1242b17f-104d-4998-8412-588e272cb071",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset ptb_text_only (/root/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n",
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /root/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-6d72a72d5c72be3d.arrow and /root/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-3e2d0e2b1466183a.arrow\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ptb_dataset = load_dataset(\"ptb_text_only\", split=\"train\")\n",
        "\n",
        "# splitting dataset in train/test (to be later used for language model evaluation)\n",
        "ptb_dataset = ptb_dataset.train_test_split(test_size=0.2, seed=1)\n",
        "ptb_train, ptb_test = ptb_dataset['train'], ptb_dataset['test']"
      ],
      "id": "69afb9de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20589cd0"
      },
      "source": [
        "#### Let's have a look at a few samples of the training dataset (and also the structure of the dataset)"
      ],
      "id": "20589cd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6ceb684",
        "outputId": "cde72a77-da48-440e-bfc5-bd5016957ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': \"a former executive agreed that the departures do n't reflect major problems adding if you see any company that grows as fast as reebok did it is going to have people coming and going\"}\n",
            "\n",
            "{'sentence': 'with talk today of a second economic <unk> in west germany east germany no longer can content itself with being the economic star in a loser league'}\n",
            "\n",
            "{'sentence': 'transportation secretary sam skinner who earlier fueled the anti-takeover fires with his <unk> attacks on foreign investment in u.s. carriers now says the bill would further <unk> the jittery capital markets'}\n"
          ]
        }
      ],
      "source": [
        "print(f\"{ptb_train[0]}\\n\\n{ptb_train[1]}\\n\\n{ptb_train[2]}\")"
      ],
      "id": "b6ceb684"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07207b13"
      },
      "source": [
        "During generation with a given language model, we often need to have a `<stop>` token in our vocabulary to terminate the generation of a given sentence/paragraph. In this dataset, every sample is a sentence, and the `<stop>` token should be added to the end of every sample (i.e., end of sentence).\n",
        "\n",
        "#### Create a new train/test dataset starting from `ptb_train` and `ptb_test` that has a `<stop>` at the end of each sentence. (Note: do not change the structure of the datasets objects, and just change the respective sentences as discussed).\n",
        "Hint: use the `.map()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#map]))."
      ],
      "id": "07207b13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa1b7b70",
        "outputId": "8c7e4832-d9e5-4ece-82de-d516b5e00526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-109730d50553c96f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-19bd99f996293818.arrow\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def add_stop_token(input_sample: dict):\n",
        "    '''\n",
        "    args:\n",
        "        input_sample: a dict representing a sample of the dataset. (look above for the dict struture)\n",
        "    output:\n",
        "        modified_sample: modified dict adding <stop> at the end of each sentence.\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    modified_sample = deepcopy(input_sample)\n",
        "    modified_sample['sentence']+=' <stop>'\n",
        "    return modified_sample\n",
        "\n",
        "\n",
        "ptb_cleaned_train = ptb_train.map(add_stop_token)\n",
        "ptb_cleaned_test = ptb_test.map(add_stop_token)"
      ],
      "id": "aa1b7b70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d338821"
      },
      "source": [
        "For the both `ptb_train` and `ptb_test` datasets, filter out every sample that has less than 3 tokens. it will help remove very short sentences that are not very helpful for training/evaluating a langugage model.\n",
        "\n",
        "Hint: use `.filter()` functionality of the `datasets` package (read more [here](https://huggingface.co/docs/datasets/process#select-and-filter))."
      ],
      "id": "2d338821"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c0222f5",
        "outputId": "8cf9fe10-f3ff-45dd-bca0-980bb4bd54ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-3d8a5e6bb1d69e67.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-c0ddb8f7c2367dad.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': \"a former executive agreed that the departures do n't reflect major problems adding if you see any company that grows as fast as reebok did it is going to have people coming and going\"}\n",
            "\n",
            "{'sentence': 'with talk today of a second economic <unk> in west germany east germany no longer can content itself with being the economic star in a loser league'}\n",
            "\n",
            "{'sentence': 'transportation secretary sam skinner who earlier fueled the anti-takeover fires with his <unk> attacks on foreign investment in u.s. carriers now says the bill would further <unk> the jittery capital markets'}\n"
          ]
        }
      ],
      "source": [
        "print(f\"{ptb_train[0]}\\n\\n{ptb_train[1]}\\n\\n{ptb_train[2]}\")\n",
        "cleaned_train_dataset = ptb_cleaned_train.filter(lambda s: True if len(s['sentence'].split(' ')) >= 3 else False)\n",
        "cleaned_test_dataset = ptb_cleaned_test.filter(lambda s: True if len(s['sentence'].split(' ')) >= 3 else False)"
      ],
      "id": "9c0222f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2733f77b"
      },
      "source": [
        "#### What are the 10 most frequent tokens in this dataset? Can you spot the token used to replace the numbers in this dataset? How are rare tokens replaced in this dataset?"
      ],
      "id": "2733f77b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28e93fb6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from collections import Counter\n",
        "def count_appearnces(counter,dataset):\n",
        "  for s in dataset:\n",
        "    counter.update(s['sentence'].split(' '))\n",
        "  return counter\n",
        "\n",
        "c_train = count_appearnces(Counter(),cleaned_train_dataset)\n",
        "c_test = count_appearnces(Counter(),cleaned_test_dataset)"
      ],
      "id": "28e93fb6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lLJgyHySwVgK"
      },
      "source": [
        "The 'N' token is used to represent numbers as can be seen by the following sentence. We can also notice that 'N' appeared 23912 times in the text which suggests it is a general number replacement since usually 'N' is not a common word in english."
      ],
      "id": "lLJgyHySwVgK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pytozOrwVgK",
        "outputId": "e0a4b61f-f104-449c-f33f-6ecd7d342260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': \"separately the company 's board adopted a proposal to <unk> its N shareholder rights plan further <unk> the company from takeover\"}\n",
            "25966\n"
          ]
        }
      ],
      "source": [
        "print(f\"{ptb_train[3]}\")\n",
        "print(c_train['N'])"
      ],
      "id": "-pytozOrwVgK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKeSjB7-wVgK",
        "outputId": "fc0ac9d2-87d5-48ac-be15-4b869b1441cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most 10 common tokens are:\n",
            "[('the', 40616), ('<unk>', 35888), ('<stop>', 33539), ('N', 25966), ('of', 19459), ('to', 18896), ('a', 16901), ('in', 14473), ('and', 14013), (\"'s\", 7850)]\n"
          ]
        }
      ],
      "source": [
        "sorted_c = sorted(c_train.items(), key=lambda x: x[1])\n",
        "print(\"The most 10 common tokens are:\\n{0}\".format(sorted_c[-10:][::-1]))"
      ],
      "id": "CKeSjB7-wVgK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "641578b1"
      },
      "source": [
        "## Task B: Fixed-Window Neural Language Models <a name='fixed_window_neural_lm'></a>"
      ],
      "id": "641578b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88cee765"
      },
      "source": [
        "This language model take as input a constant number of tokens, and then outputs a probability distribution for the next token. In this section, we assume the underlying model is a FeedForward Network (FFN) with a single hidden layer. This model doesn't have the sparsity issue of N-gram language models, but is always limited to a fixed window of tokens.\n",
        "\n",
        "In this section, we don't include the training of the model but rather we use a pretrained model on the same training dataset. We evaluate the language model over the `ptb_test` dataset, to show the power of neural language models, when compared to N-gram language models.\n",
        "\n",
        "More importantly, we use PyTorch modules in this section, so that you get more familiar with its capabilities. Throughout this exercise, we use a `window_size=3` for this model.\n",
        "\n"
      ],
      "id": "88cee765"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6808da1e"
      },
      "source": [
        "Let's first create a dataset of all consecutive tokens of length `window_size` from the `ptb_train` dataset. you can read more about PyTorch datasets and how to create a custom dataset  [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."
      ],
      "id": "6808da1e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70f5c3bf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "window_size = 3\n",
        "vocabulary_size = 10000\n",
        "word_emb_dim = 100\n",
        "hidden_dim = 100\n",
        "\n",
        "class FixedWindowDataset(Dataset):\n",
        "    # read more about custom datasets at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "    def __init__(self,\n",
        "                 train_dataset: datasets.arrow_dataset.Dataset,\n",
        "                 test_dataset: datasets.arrow_dataset.Dataset,\n",
        "                 window_size: int,\n",
        "                 vocabulary_size: int\n",
        "                ):\n",
        "        self.prepared_train_dataset = self.prepare_fixed_window_lm_dataset(train_dataset, window_size + 1)\n",
        "        self.prepared_test_dataset = self.prepare_fixed_window_lm_dataset(test_dataset, window_size + 1)\n",
        "\n",
        "        dataset_vocab = self.get_dataset_vocabulary(train_dataset)\n",
        "        # defining a dictionary that simply maps tokens to their respective index in the embedding matrix\n",
        "        self.word_to_index = {word: idx for idx,word in enumerate(dataset_vocab)}\n",
        "        self.index_to_word = {idx: word for idx,word in enumerate(dataset_vocab)}\n",
        "        assert vocabulary_size > len(dataset_vocab) , f\"The dataset vocab size is {len(dataset_vocab)}!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prepared_train_dataset)\n",
        "\n",
        "    def get_encoded_test_samples(self):\n",
        "        all_token_lists = [sample.split() for sample in self.prepared_test_dataset]\n",
        "        all_token_ids = [[self.word_to_index.get(word, self.word_to_index[\"<unk>\"])\n",
        "                          for word in token_list[:-1]]\n",
        "                         for token_list in all_token_lists\n",
        "                        ]\n",
        "        all_next_token_ids = [self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"])\n",
        "                              for token_list in all_token_lists]\n",
        "        return torch.tensor(all_token_ids).to(device), torch.tensor(all_next_token_ids).to(device)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # here we need to transform the data to the format we expect at the model input\n",
        "        token_list = self.prepared_train_dataset[idx].split()\n",
        "        # having a fallback to <unk> token if an unseen word is encoded.\n",
        "        token_ids = [self.word_to_index.get(word, self.word_to_index[\"<unk>\"]) for word in token_list[:-1]]\n",
        "        next_token_id = self.word_to_index.get(token_list[-1], self.word_to_index[\"<unk>\"])\n",
        "        return torch.tensor(token_ids).to(device), torch.tensor(next_token_id).to(device)\n",
        "\n",
        "    def decode_idx_to_word(self, token_id):\n",
        "        return [self.index_to_word[id_.item()] for id_ in token_id]\n",
        "\n",
        "    def get_dataset_vocabulary(self, train_dataset: datasets.arrow_dataset.Dataset):\n",
        "        vocab = sorted(set(\" \".join([sample[\"sentence\"] for sample in train_dataset]).split()))\n",
        "        # we also add a <start> token to include initial tokens in the sentences in the dataset\n",
        "        vocab += [\"<start>\"]\n",
        "        return vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_fixed_window_lm_dataset(target_dataset: datasets.arrow_dataset.Dataset,\n",
        "                                        window_size: int):\n",
        "        '''\n",
        "        Please note that for the very first tokens, they will be added like \"<start> <start> Token#1\".\n",
        "        args:\n",
        "            target_dataset: the target dataset where its consecutive tokens of length 'window_size' should be extracted\n",
        "            window_size: the window size for the language model\n",
        "        output:\n",
        "            prepared_dataset: a list of strings each containing 'window_size' tokens.\n",
        "        '''\n",
        "\n",
        "        prepared_dataset = []\n",
        "        for s in target_dataset:\n",
        "          prevs = ['<start>']*(window_size-1)\n",
        "          for w in s['sentence'].split(' '):\n",
        "            first_words = \" \".join(prevs)\n",
        "            prepared_dataset.append('{0} {1}'.format(first_words, w))\n",
        "            prevs = prevs[1:] + [w]\n",
        "\n",
        "        return prepared_dataset\n",
        "\n",
        ""
      ],
      "id": "70f5c3bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "524f4a18"
      },
      "outputs": [],
      "source": [
        "fixed_window_dataset = FixedWindowDataset(ptb_train, ptb_test, window_size, vocabulary_size)\n",
        "\n",
        "# let's create a simple dataloader for this dataset\n",
        "train_dataloader =  DataLoader(fixed_window_dataset, batch_size=8, shuffle=True)\n"
      ],
      "id": "524f4a18"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b44e90d"
      },
      "source": [
        "Now, let's define the underlying PyTorch model for the language model. You can read more about PyTorch models [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n",
        "\n",
        "**Note**: Here in the forward pass, we compute the negative log-likelihood after passing through the FFN layers. Here we use `torch.nn.LogSoftmax`, as it's numerically more stable than doing seperately `softmax` followed by taking its logarithm."
      ],
      "id": "5b44e90d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9373555c-97cb-407d-94c4-d7016c05d5b7"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class Fixed_window_language_model(torch.nn.Module):\n",
        "    def __init__(self, emb_dim, hidden_dim, window_size, vocab_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.word_embeddings = torch.nn.Embedding(vocab_size, emb_dim) # word embeddings\n",
        "        self.linear1 = torch.nn.Linear(window_size * emb_dim, hidden_dim) # first linear layer\n",
        "        self.activation_func = torch.tanh # the activation function\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, vocab_size) # second linear layer\n",
        "\n",
        "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.criterion = torch.nn.NLLLoss()\n",
        "\n",
        "    def forward(self, input_ids, labels, softmax=False):\n",
        "        inputs_embeds = self.word_embeddings(input_ids)\n",
        "        concat_input_embed = inputs_embeds.reshape(-1, self.emb_dim * self.window_size)\n",
        "        hidden_state = self.activation_func( self.linear1(concat_input_embed) )\n",
        "\n",
        "        logits = self.log_softmax(self.linear2(hidden_state))\n",
        "        loss = self.criterion(logits, labels)\n",
        "\n",
        "        return loss"
      ],
      "id": "9373555c-97cb-407d-94c4-d7016c05d5b7"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class FixedPredictionModel(Fixed_window_language_model):\n",
        "    def __init__(self, emb_dim, hidden_dim, window_size, vocab_size=10000):\n",
        "        super().__init__(emb_dim, hidden_dim, window_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def predict(self, input_ids, labels):\n",
        "        inputs_embeds = self.word_embeddings(input_ids)\n",
        "        concat_input_embed = inputs_embeds.reshape(-1, self.emb_dim * self.window_size)\n",
        "        hidden_state = self.activation_func( self.linear1(concat_input_embed) )\n",
        "        out = self.linear2(hidden_state)\n",
        "        Px = self.softmax(out)\n",
        "        return Px"
      ],
      "metadata": {
        "id": "kT2vXrSxqEki"
      },
      "id": "kT2vXrSxqEki",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dae5397"
      },
      "source": [
        "Now let's see how easy it is to train a model with PyTorch! (we provide a trained model in the cell after train, so that you can just start using the model without going through the time-consuming training)"
      ],
      "id": "4dae5397"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93779901"
      },
      "outputs": [],
      "source": [
        "# defining the model\n",
        "# model_fixed_window = Fixed_window_language_model(emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
        "#                                                  window_size=window_size, vocab_size=vocabulary_size).to(device)\n",
        "\n",
        "# defining the model\n",
        "model_fixed_window = FixedPredictionModel(emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
        "                                                 window_size=window_size, vocab_size=vocabulary_size).to(device)\n",
        "\n",
        "\n",
        "# defining the optimizer\n",
        "optimizer = optim.SGD(model_fixed_window.parameters(),\n",
        "                      lr=0.005,\n",
        "                      momentum=0.9)"
      ],
      "id": "93779901"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77626d29",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea608dec-9ff2-4661-db01-d41028c49172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "def train_fixed_window():\n",
        "  for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        # get the inputs; data is a tuple of (context, target)\n",
        "        context, target = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        loss = model_fixed_window(context, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 5000 == 4999. :    # print every 5000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 5000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "# train_fixed_window()\n",
        "print('Finished Training')\n",
        "\n",
        "# saving the trained model\n",
        "# torch.save(model_fixed_window.state_dict(), \"fixed_window_model.pt\")"
      ],
      "id": "77626d29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0483dd01"
      },
      "source": [
        "We provide a trained model, so that you can start using it right away"
      ],
      "id": "0483dd01"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54ba51b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5dbaa6e-fa6e-4241-c2da-fb84fb43d659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "fixed_window_checkpoint_file = \"fixed_window_model.pt\"\n",
        "model_fixed_window.load_state_dict(torch.load(fixed_window_checkpoint_file))"
      ],
      "id": "54ba51b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b806f375"
      },
      "outputs": [],
      "source": [
        "# context and 'target' ids (target is the next word after the context)\n",
        "test_token_ids, test_target_ids = fixed_window_dataset.get_encoded_test_samples()"
      ],
      "id": "b806f375"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f87e283d"
      },
      "source": [
        "We now have the `test_token_ids`, `test_target_ids` tensors for the test dataset. The `test_token_ids` are the context ids and `test_target_ids` are the respective **next token** (a.k.a. target here) for these contexts.\n",
        "#### Using the trained model, implement a function that can output the loss for the discussed test dataset. How can we generally decide if the model is overfitted to the train dataset or not?"
      ],
      "id": "f87e283d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e352e019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc151cf-94e9-41fc-a8ee-8609d56369cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset loss is 1.8215297501377739\n"
          ]
        }
      ],
      "source": [
        "def generate_test_dataset_loss(model: torch.nn.Module,\n",
        "                               test_token_ids: torch.Tensor,\n",
        "                               test_target_ids: torch.Tensor):\n",
        "    '''\n",
        "    args:\n",
        "        model: fixed-window language model\n",
        "        test_token_ids: the context ids in a single tensor.\n",
        "        test_target_ids: the target ids (next token after the context) in a single tensor.\n",
        "    output:\n",
        "        avg_test_loss: The average loss of model over test dataset.\n",
        "    '''\n",
        "\n",
        "    batch_size = 4\n",
        "    test_loss = []\n",
        "\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    class MyDataset(Dataset):\n",
        "      def __init__(self, input_data, labels):\n",
        "          self.input_data = input_data\n",
        "          self.labels = labels\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.input_data)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          input_sample = self.input_data[idx]\n",
        "          label = self.labels[idx]\n",
        "          return input_sample, label\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    test_ds = MyDataset(test_token_ids.to(device), test_target_ids.to(device))\n",
        "    test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for idx, (inputs, labels) in enumerate(test_dl):\n",
        "      loss = model(inputs, labels)\n",
        "      test_loss.append(loss.item())\n",
        "\n",
        "    batch_avg = np.array(test_loss).mean()\n",
        "\n",
        "    # Average loss per sample\n",
        "    return batch_avg/batch_size\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "test_dataset_loss = generate_test_dataset_loss(model_fixed_window, test_token_ids, test_target_ids)\n",
        "print(f\"Test dataset loss is {test_dataset_loss}\")"
      ],
      "id": "e352e019"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "085d7476"
      },
      "source": [
        "#### Using the trained fixed-window model, implemention a function that can output entropy for a given sequence."
      ],
      "id": "085d7476"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db54e454"
      },
      "outputs": [],
      "source": [
        "def get_seqeuence_entropy_fixed_window_lm(model: torch.nn.Module,\n",
        "                                              input_sequence: str,\n",
        "                                              window_size: int,\n",
        "                                              word_to_idx: dict):\n",
        "    '''\n",
        "    Note that e.g., in order to get the first token probability, you need to pass a sequence\n",
        "    like \"<start> <start> <start>\" (prefix padding) to the neural model. In a similar fashion, we need to pass\n",
        "    \"<start> <start> TOKEN#1\" for getting the probability of the second token.\n",
        "    args:\n",
        "        model: fixed-window language model\n",
        "        input_sequence: the sequence for which we want to calculate the probability\n",
        "        window_size: the size of window for the language model\n",
        "        word_to_idx: a mapping from words to the embedding indices (to encode tokens before being\n",
        "                     passed to model). You can get this dict from 'fixed_window_dataset.word_to_index'\n",
        "    output:\n",
        "        sequence_entropy: the entropy for the input sequence using the trained model\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    input_dict = {'sentence': input_sequence}\n",
        "    sub_sequences = FixedWindowDataset.prepare_fixed_window_lm_dataset([input_dict], window_size=4)\n",
        "    entropy_sum=0\n",
        "    for n_word_seq in sub_sequences:\n",
        "        # pdb.set_trace()\n",
        "        # here we need to transform the data to the format we expect at the model input\n",
        "        token_list = n_word_seq.split()\n",
        "\n",
        "        # having a fallback to <unk> token if an unseen word is encoded.\n",
        "        token_ids = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in token_list[:-1]]\n",
        "        next_token_id = word_to_idx.get(token_list[-1], word_to_idx[\"<unk>\"])\n",
        "\n",
        "        x, y = torch.tensor(token_ids).to(device), torch.tensor([next_token_id]).to(device)\n",
        "        Px = model.predict(x, y)\n",
        "        entropy_sum+=-(Px*torch.log(Px)).sum().item()\n",
        "\n",
        "    return entropy_sum/len(sub_sequences)"
      ],
      "id": "db54e454"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pdb\n",
        "word_to_idx = fixed_window_dataset.word_to_index\n",
        "window_size = 4\n",
        "sentence = ptb_test[0]['sentence']\n",
        "\n",
        "sentence_ent = get_seqeuence_entropy_fixed_window_lm(model_fixed_window, sentence ,window_size,word_to_idx)\n",
        "print(\"Sentence: \\\"{0}\\\"\\n has entropy of {1} with window size of {2}\".format(sentence,sentence_ent, window_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAdJTuURko7j",
        "outputId": "46025c77-baf5-44b1-a6b8-9ea7b22b80c6"
      },
      "id": "QAdJTuURko7j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"jefferies group inc. said third-quarter net income fell N N to $ N million or N cents a share from $ N million or N cents a share on more shares a year earlier\"\n",
            " has entropy of 4.592703431406441 with window size of 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ptb_test[0]\n",
        "# fixed_window_dataset.get_encoded_test_samples()[1].shape\n",
        "len(ptb_test)"
      ],
      "metadata": {
        "id": "NwUeB__JccBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2032fcaf-ced1-4406-9cdb-5fa85e1db531"
      },
      "id": "NwUeB__JccBE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8414"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d055835"
      },
      "source": [
        "#### Compute the perplexity for the trained fixed-window language model over `ptb_test` dataset using the previous function."
      ],
      "id": "7d055835"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4289886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1b6cfa-98f1-47b0-b2f2-153710a3366b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting perplexity calculation\n",
            "0.0\n",
            "0.04753981459472308\n",
            "0.09507962918944617\n",
            "0.14261944378416924\n",
            "0.19015925837889233\n",
            "0.2376990729736154\n",
            "0.2852388875683385\n",
            "0.33277870216306155\n",
            "0.38031851675778466\n",
            "0.4278583313525077\n",
            "0.4753981459472308\n",
            "0.5229379605419539\n",
            "0.570477775136677\n",
            "0.6180175897314001\n",
            "0.6655574043261231\n",
            "0.7130972189208462\n",
            "0.7606370335155693\n",
            "0.8081768481102923\n",
            "0.8557166627050155\n",
            "0.9032564772997386\n",
            "0.9507962918944616\n",
            "0.9983361064891847\n",
            "The fixed-window model perplexity over test dataset is 42.749297469868104\n"
          ]
        }
      ],
      "source": [
        "entropy = 0\n",
        "test_len = len(ptb_test)\n",
        "print(\"starting perplexity calculation\")\n",
        "for i,test_dict in enumerate(ptb_test):\n",
        "  sentence = test_dict['sentence']\n",
        "  entropy += get_seqeuence_entropy_fixed_window_lm(model_fixed_window, sentence ,window_size,word_to_idx)\n",
        "  if i%400 == 0:\n",
        "    print(i/test_len)\n",
        "avg_entropy = entropy/test_len\n",
        "perplexity = 2**avg_entropy\n",
        "print(f\"The fixed-window model perplexity over test dataset is {perplexity}\")"
      ],
      "id": "f4289886"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "240c3d89"
      },
      "source": [
        "### Task C: RNN-based Language Model <a name='rnn_lm'></a>\n",
        "To address the need for a neural architecture that can proceed with any length input (as opposed to the fixed-window model that can only process a fixed number of tokens), we implement the Recurrent Neural Network (RNN). The core idea behind is that we can apply the same weight W repeatedly.\n",
        "\n",
        "An advatange of RNN model compared to fixed-window langauage model is that we can pass a given sentence at once, instead of passing it in many windows of size `window_size`. Moreover, the language model has the ability to look behind further that a fixed number of tokens.\n",
        "\n",
        " As we already did a neural model training exercise for the previous neural model, we only provide a trained LM at this section, so that you can focus only on the analysis part.\n",
        "\n",
        "You can find the dataset structure as well as the RNN architecture in the `rnn_utils.py` file."
      ],
      "id": "240c3d89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc631bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d69b4a-c681-4309-9faf-555ed48d826b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "My_RNN_language_model(\n",
              "  (criterion): CrossEntropyLoss()\n",
              "  (embedding): Embedding(10000, 200)\n",
              "  (rnn): RNN(200, 200, num_layers=4)\n",
              "  (dropout): Dropout(p=0.001, inplace=False)\n",
              "  (lm_decoder): Linear(in_features=200, out_features=10000, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "\n",
        "from rnn_utils import RNNDataset, RNN_language_model\n",
        "\n",
        "class My_RNN_language_model(RNN_language_model):\n",
        "  def __init__(self, vocab_size, emb_dim, hidden_dim, dropout=0.001, pad_idx = -1):\n",
        "        super().__init__(vocab_size, emb_dim, hidden_dim, dropout=0.001, pad_idx = -1)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "  def predict(self, context):\n",
        "        context = context.t() # transposing it for RNN model\n",
        "        #context = [src len, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(context))\n",
        "\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        #outputs = [src len, batch size, hidden_dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        outputs = self.lm_decoder(outputs.permute(1, 0, 2))[:, :-1, :].permute(0, 2, 1)\n",
        "        outputs = self.softmax(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "vocabulary_size = 10000\n",
        "word_emb_dim = 200\n",
        "hidden_dim = 200\n",
        "\n",
        "rnn_dataset = RNNDataset(ptb_train, ptb_test, vocabulary_size)\n",
        "\n",
        "# if gpu is available, we puts the model on it\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Here we need a <pad> token for the RNN model, in order to have a batch of sequences with difference sizes\n",
        "pad_idx = rnn_dataset.pad_idx # the index for <pad> token\n",
        "rnn_model = My_RNN_language_model(vocab_size=vocabulary_size, emb_dim=word_emb_dim, hidden_dim=hidden_dim,\n",
        "                               pad_idx=pad_idx)\n",
        "rnn_model.to(device)\n",
        "rnn_model.eval()"
      ],
      "id": "fc631bc1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25137a9f"
      },
      "source": [
        "load the model weights using the state_dict in `rnn_model.pt` file."
      ],
      "id": "25137a9f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52adb1f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c06955b-3451-4435-abf1-34b433b5c25d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "rnn_checkpoint_file = \"rnn_model.pt\"\n",
        "rnn_model.load_state_dict(torch.load(rnn_checkpoint_file))"
      ],
      "id": "52adb1f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f06eb967"
      },
      "source": [
        "As the training of an RNN model is time-consuming, we provide a trained language model on this dataset (`rnn_model.pt`), so that you can just analyze the model performance here.\n",
        "As mentioned above, as RNN can get sequences with varying lengths, the input sequences should be padded with a special token like `<pad>`, so that we can create a batch of sentences. The output of the defined RNN model (see the architecture detail `rnn_utils.py`) is the model's entropy over the input data.\n",
        "\n",
        "#### First get the encoded test samples of `ptb_test` dataset, and then pass these (already padded) sentences to the RNN model to get the respective entropy values. Compute the perplexity of the model and compare it with previous approaches.\n",
        "**HINT**: You can use the `get_encoded_test_samples` function of `rnn_dataset` to get encoded test samples.\n"
      ],
      "id": "f06eb967"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09d7b99f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1569c1d3-36e4-4775-db2a-b415c7fbe9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00\n",
            "0.06\n",
            "0.12\n",
            "0.18\n",
            "0.24\n",
            "0.30\n",
            "0.36\n",
            "0.42\n",
            "0.48\n",
            "0.53\n",
            "0.59\n",
            "0.65\n",
            "0.71\n",
            "0.77\n",
            "0.83\n",
            "0.89\n",
            "0.95\n",
            "The model perplexity is 58.556497632025945\n"
          ]
        }
      ],
      "source": [
        "def get_seqeuence_entropy_rnn_lm(model: torch.nn.Module,\n",
        "                                              input_sequence: str,\n",
        "                                              window_size: int,\n",
        "                                              word_to_idx: dict):\n",
        "    '''\n",
        "    Note that e.g., in order to get the first token probability, you need to pass a sequence\n",
        "    like \"<start> <start> <start>\" (prefix padding) to the neural model. In a similar fashion, we need to pass\n",
        "    \"<start> <start> TOKEN#1\" for getting the probability of the second token.\n",
        "    args:\n",
        "        model: fixed-window language model\n",
        "        input_sequence: the sequence for which we want to calculate the probability\n",
        "        window_size: the size of window for the language model\n",
        "        word_to_idx: a mapping from words to the embedding indices (to encode tokens before being\n",
        "                     passed to model). You can get this dict from 'fixed_window_dataset.word_to_index'\n",
        "    output:\n",
        "        sequence_entropy: the entropy for the input sequence using the trained model\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    input_dict = {'sentence': input_sequence}\n",
        "    sub_sequences = FixedWindowDataset.prepare_fixed_window_lm_dataset([input_dict], window_size=4)\n",
        "    entropy_sum=0\n",
        "    for n_word_seq in sub_sequences:\n",
        "        # pdb.set_trace()\n",
        "        # here we need to transform the data to the format we expect at the model input\n",
        "        token_list = n_word_seq.split()\n",
        "\n",
        "        # having a fallback to <unk> token if an unseen word is encoded.\n",
        "        token_ids = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in token_list[:-1]]\n",
        "        next_token_id = word_to_idx.get(token_list[-1], word_to_idx[\"<unk>\"])\n",
        "\n",
        "        x, y = torch.tensor(token_ids).to(device), torch.tensor([next_token_id]).to(device)\n",
        "        Px = model.predict(x, y)\n",
        "        entropy_sum+=-(Px*torch.log(Px)).sum().item()\n",
        "\n",
        "    return entropy_sum/len(sub_sequences)\n",
        "test_perplexity = -1\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "def rnn_entropy():\n",
        "\n",
        "  entropy_sum = 0\n",
        "  test_samples = rnn_dataset.get_encoded_test_samples()\n",
        "  test_len = len(test_samples)\n",
        "  for i, test_sample in enumerate(test_samples):\n",
        "\n",
        "    sample = test_sample.reshape(1,-1).to(device)\n",
        "    output = rnn_model.predict(sample)\n",
        "\n",
        "    entropy_sum +=-float(torch.sum(output*torch.log(output))/output.shape[2])\n",
        "    if i%500 == 0:\n",
        "      print(\"{:.2f}\".format(i/test_len))\n",
        "  return entropy_sum/test_len\n",
        "entropy = rnn_entropy()\n",
        "test_perplexity = 2**entropy\n",
        "print(f\"The model perplexity is {test_perplexity}\")"
      ],
      "id": "09d7b99f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a321bbc-352f-46a8-b05d-5174accbdc54"
      },
      "source": [
        "### Task D: MLM Transformer Language Models (Bonus Question: 10 pts) <a name='rnn_lm'></a>\n",
        "\n",
        "We are here interested in computing the perplexity of MLM Transformer Language Models such as BERT and RoBERTa. Hoewever, the perplexity for MLM models is not well-defined (The difference with GPT models is illstrated [here](https://huggingface.co/docs/transformers/perplexity).\n",
        "\n",
        "Instructions: First clone the following repository: https://github.com/asahi417/lmppl.\n",
        "Install the requirements and follow the instructions to compute the pseudo-perplexity [(Wang and Cho, 2019)](https://aclanthology.org/W19-2304.pdf) of 'BERT-base-uncased', 'BERT-large-uncased', 'RoBERTa-base' and 'RoBERTa-large' for the sentences:\n",
        "'Shelly ate the sliced banana with a fork.' and 'The fork of Shelly ate the sliced banana.'.\n",
        "\n",
        "Which sentence gets the lowest pseudo-perplexity for each of the models? Which is the best model according to this test?\n",
        "What is the relation of this test to semantic roles?\n"
      ],
      "id": "4a321bbc-352f-46a8-b05d-5174accbdc54"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "30306e60-a39b-4a5d-8091-de8244bb7aa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cc2a9f-188d-4618-cd72-d25bcfa55f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lmppl in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lmppl) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lmppl) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lmppl) (2.27.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from lmppl) (4.30.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from lmppl) (0.1.99)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from lmppl) (0.20.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from lmppl) (0.27.8)\n",
            "Requirement already satisfied: protobuf<3.20 in /usr/local/lib/python3.10/dist-packages (from lmppl) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->lmppl) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->lmppl) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->lmppl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->lmppl) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lmppl) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->lmppl) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lmppl) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lmppl) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lmppl) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->lmppl) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lmppl) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lmppl) (16.0.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai->lmppl) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lmppl) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lmppl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->lmppl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lmppl) (3.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->lmppl) (0.15.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->lmppl) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->lmppl) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->lmppl) (0.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->lmppl) (2023.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->lmppl) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->lmppl) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->lmppl) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->lmppl) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->lmppl) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->lmppl) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lmppl) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lmppl) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#!git clone https://github.com/asahi417/lmppl\n",
        "!pip install lmppl\n",
        "sentence1 = 'Shelly ate the sliced banana with a fork.'\n",
        "sentence2 = 'The fork of Shelly ate the sliced banana.'"
      ],
      "id": "30306e60-a39b-4a5d-8091-de8244bb7aa1"
    },
    {
      "cell_type": "code",
      "source": [
        "scorers = {}\n",
        "scorers['RoBERTa_base_sc'] = lmppl.MaskedLM('RoBERTa-base', max_length=100)\n",
        "scorers['BERT_large_uncased'] = lmppl.MaskedLM('BERT-large-uncased', max_length=100)\n",
        "scorers['RoBERTa_large'] = lmppl.MaskedLM('RoBERTa-large', max_length=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkNqUEPn6xXB",
        "outputId": "2e23833f-9f40-40db-911d-b676523d118c"
      },
      "id": "wkNqUEPn6xXB",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at BERT-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lmppl\n",
        "import lmppl\n",
        "text = [sentence1, sentence2]\n",
        "\n",
        "for scorer_name, scorer in scorers.items():\n",
        "  ppl = scorer.get_perplexity(text)\n",
        "  print(\"\\n########## {0} ##########\".format(scorer_name))\n",
        "  print(\"{0} has Psuedo-Perplexity of {1}\".format(text[0], ppl[0]))\n",
        "  print(\"{0} has Psuedo-Perplexity of {1}\\n\".format(text[1], ppl[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAX3WuPd5ZBf",
        "outputId": "44b52ba3-5dea-4c38-e349-83e7093f6685"
      },
      "id": "KAX3WuPd5ZBf",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "########## RoBERTa_base_sc ##########\n",
            "Shelly ate the sliced banana with a fork. has Psuedo-Perplexity of 13.817431435935024\n",
            "The fork of Shelly ate the sliced banana. has Psuedo-Perplexity of 31.41023792391415\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "########## BERT_large_uncased ##########\n",
            "Shelly ate the sliced banana with a fork. has Psuedo-Perplexity of 22.458738502630812\n",
            "The fork of Shelly ate the sliced banana. has Psuedo-Perplexity of 199.5484317656372\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "########## RoBERTa_large ##########\n",
            "Shelly ate the sliced banana with a fork. has Psuedo-Perplexity of 13.226297032073228\n",
            "The fork of Shelly ate the sliced banana. has Psuedo-Perplexity of 42.13932649615006\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55399127-3c70-4a71-9dc4-a591f9691ef3"
      },
      "source": [
        "# YOUR ANSWERS HERE"
      ],
      "id": "55399127-3c70-4a71-9dc4-a591f9691ef3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer\n",
        "1. The sentence \"Shelly ate the sliced banana with a fork.\" has a lower perplexity, indicating that it is more comprehensible to the average reader. Consequently, it is expected to achieve a lower perplexity score.\n",
        "\n",
        "2. Based on this evaluation, the RoBERTa_large model demonstrates superior performance.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "RoBERTa_large surpasses RoBERTa_base_sc in terms of the number of parameters it possesses. With its increased size, RoBERTa_large has the potential to capture more intricate language patterns and dependencies, resulting in improved perplexity performance.\n",
        "\n",
        "RoBERTa_base_sc has undergone additional fine-tuning specifically for sentence classification tasks, suggesting that RoBERTa_large may be more suitable for different types of tasks.\n",
        "\n",
        "In general, RoBERTa represents an enhanced iteration of BERT, thereby making it logical for BERT to exhibit the lowest perplexity among the models considered.\n",
        "\n",
        "3. What is the relation of this test to semantic roles?\n",
        "\n",
        "In terms of semantic roles, the fork couldn't eat a bannana. It is an AGENT which cannot perform an action.\n"
      ],
      "metadata": {
        "id": "URtMJ1aU_bwg"
      },
      "id": "URtMJ1aU_bwg"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8ced340a52f9326f5856e1d63a73f97bd9f0a225610b549ff7b502d766a19ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}